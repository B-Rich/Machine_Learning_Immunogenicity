{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(0, '/home/ubuntu/roger/Machine_Learning_Immunogenicity/src')\n",
    "import onehot\n",
    "from gzip import GzipFile\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/home/roger/Machine_Learning_Immunogenicity'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(278230, 27)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcell_all=pd.read_table(\"/home/roger/other/Machine_Learning_Immunogenicity/data/tcell.txt.gz\",compression='gzip')\n",
    "tcell_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tcell_comp=pd.read_table('/home/roger/other/Machine_Learning_Immunogenicity/data/tcell_peptide_allele_nodups.txt.gz',compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77970, 3)\n"
     ]
    }
   ],
   "source": [
    "print tcell_comp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Allele Name</th>\n",
       "      <th>bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KLEDLERDL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IEQQADNMITEMLQK</td>\n",
       "      <td>H2-IAk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGEGWPYIACRTSVVGRAWE</td>\n",
       "      <td>H2-IAb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRVAFAGL</td>\n",
       "      <td>H2-Kb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVAKAGKPL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AMLQDIATL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KMLRGVNVL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAVEELKAL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VEGEALATL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AEWPTFNVGW</td>\n",
       "      <td>HLA-B*44:03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Description  Allele Name  bin\n",
       "0             KLEDLERDL  HLA-A*02:01    1\n",
       "1       IEQQADNMITEMLQK       H2-IAk    1\n",
       "2  SGEGWPYIACRTSVVGRAWE       H2-IAb    1\n",
       "3              TRVAFAGL        H2-Kb    1\n",
       "4             AVAKAGKPL  HLA-E*01:01    1\n",
       "5             AMLQDIATL  HLA-E*01:01    1\n",
       "6             KMLRGVNVL  HLA-E*01:01    1\n",
       "7             AAVEELKAL  HLA-E*01:01    0\n",
       "8             VEGEALATL  HLA-E*01:01    0\n",
       "9            AEWPTFNVGW  HLA-B*44:03    1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcell_comp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9,\n",
       " 15,\n",
       " 20,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 15,\n",
       " 14,\n",
       " 10,\n",
       " 11,\n",
       " 20,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 15,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 18,\n",
       " 12,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 12,\n",
       " 9,\n",
       " 15,\n",
       " 15,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 15,\n",
       " 9,\n",
       " 7,\n",
       " 9,\n",
       " 20,\n",
       " 15,\n",
       " 15,\n",
       " 23,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 20,\n",
       " 23,\n",
       " 15,\n",
       " 15,\n",
       " 23,\n",
       " 15,\n",
       " 15,\n",
       " 20,\n",
       " 15,\n",
       " 23,\n",
       " 20,\n",
       " 20,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 20,\n",
       " 15,\n",
       " 20,\n",
       " 15,\n",
       " 20,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 20,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 20,\n",
       " 10,\n",
       " 18,\n",
       " 39,\n",
       " 39,\n",
       " 14,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 16,\n",
       " 31,\n",
       " 9,\n",
       " 9,\n",
       " 13,\n",
       " 9,\n",
       " 13,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 13,\n",
       " 23,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 9,\n",
       " 9,\n",
       " 13,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 15,\n",
       " 15,\n",
       " 20,\n",
       " 15,\n",
       " 14,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 20,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 16,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 30,\n",
       " 22,\n",
       " 31,\n",
       " 30,\n",
       " 30,\n",
       " 30,\n",
       " 31,\n",
       " 25,\n",
       " 20,\n",
       " 20,\n",
       " 8,\n",
       " 9,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 20,\n",
       " 15,\n",
       " 13,\n",
       " 9,\n",
       " 9,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 10,\n",
       " 12,\n",
       " 13,\n",
       " 22,\n",
       " 20,\n",
       " 16,\n",
       " 19,\n",
       " 12,\n",
       " 24,\n",
       " 36,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 26,\n",
       " 24,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 12,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 12,\n",
       " 13,\n",
       " 12,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 17,\n",
       " 8,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 15,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 10,\n",
       " 18,\n",
       " 9,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 17,\n",
       " 9,\n",
       " 19,\n",
       " 14,\n",
       " 10,\n",
       " 10,\n",
       " 12,\n",
       " 12,\n",
       " 9,\n",
       " 17,\n",
       " 18,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 16,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 12,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 10,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 19,\n",
       " 15,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 8,\n",
       " 15,\n",
       " 9,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 10,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 11,\n",
       " 9,\n",
       " 11,\n",
       " 11,\n",
       " 9,\n",
       " 11,\n",
       " 9,\n",
       " 11,\n",
       " 10,\n",
       " 14,\n",
       " 17,\n",
       " 15,\n",
       " 15,\n",
       " 36,\n",
       " 15,\n",
       " 15,\n",
       " 18,\n",
       " 10,\n",
       " 20,\n",
       " 24,\n",
       " 25,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 18,\n",
       " 12,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 16,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 18,\n",
       " 24,\n",
       " 9,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 13,\n",
       " 12,\n",
       " 12,\n",
       " 12,\n",
       " 14,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 9,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 19,\n",
       " 16,\n",
       " 18,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 25,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 7,\n",
       " 20,\n",
       " 23,\n",
       " 17,\n",
       " 9,\n",
       " 35,\n",
       " 43,\n",
       " 32,\n",
       " 40,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 18,\n",
       " 18,\n",
       " 18,\n",
       " 19,\n",
       " 18,\n",
       " 16,\n",
       " 16,\n",
       " 14,\n",
       " 20,\n",
       " 20,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 20,\n",
       " 40,\n",
       " 40,\n",
       " 29,\n",
       " 23,\n",
       " 36,\n",
       " 25,\n",
       " 22,\n",
       " 9,\n",
       " 9,\n",
       " 15,\n",
       " 9,\n",
       " 11,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 16,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 11,\n",
       " 11,\n",
       " 11,\n",
       " 9,\n",
       " 20,\n",
       " 9,\n",
       " 20,\n",
       " 8,\n",
       " 9,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 20,\n",
       " 18,\n",
       " 8,\n",
       " 12,\n",
       " 10,\n",
       " 16,\n",
       " 16,\n",
       " 10,\n",
       " 16,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 15,\n",
       " 9,\n",
       " 14,\n",
       " 9,\n",
       " 9,\n",
       " 8,\n",
       " 17,\n",
       " 11,\n",
       " 9,\n",
       " 9,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 13,\n",
       " 20,\n",
       " 13,\n",
       " 12,\n",
       " 16,\n",
       " 16,\n",
       " 14,\n",
       " 18,\n",
       " 17,\n",
       " 13,\n",
       " 15,\n",
       " 10,\n",
       " 9,\n",
       " 11,\n",
       " 11,\n",
       " 10,\n",
       " 15,\n",
       " 10,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 19,\n",
       " 9,\n",
       " 16,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(len,tcell_comp['Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([  141, 22314, 44689,  9370,   695,   485,   113,    70,    51,\n",
      "          30,    10,     1,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     1]), array([   3.        ,    7.56666667,   12.13333333,   16.7       ,\n",
      "         21.26666667,   25.83333333,   30.4       ,   34.96666667,\n",
      "         39.53333333,   44.1       ,   48.66666667,   53.23333333,\n",
      "         57.8       ,   62.36666667,   66.93333333,   71.5       ,\n",
      "         76.06666667,   80.63333333,   85.2       ,   89.76666667,\n",
      "         94.33333333,   98.9       ,  103.46666667,  108.03333333,\n",
      "        112.6       ,  117.16666667,  121.73333333,  126.3       ,\n",
      "        130.86666667,  135.43333333,  140.        ]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEACAYAAABfxaZOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nl6SCWPqSVJD/B5HvRai07gstAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82c8597310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76134\n"
     ]
    }
   ],
   "source": [
    "print np.histogram(map(len,tcell_comp['Description']),bins=30)\n",
    "plt.hist(np.histogram(map(len,tcell_comp['Description']),bins=30))\n",
    "plt.show()\n",
    "print np.sum(np.asarray(map(len,tcell_comp['Description'])) <= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44524, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Allele Name</th>\n",
       "      <th>bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KLEDLERDL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IEQQADNMITEMLQK</td>\n",
       "      <td>H2-IAk</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGEGWPYIACRTSVVGRAWE</td>\n",
       "      <td>H2-IAb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRVAFAGL</td>\n",
       "      <td>H2-Kb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVAKAGKPL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AMLQDIATL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KMLRGVNVL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAVEELKAL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VEGEALATL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AEWPTFNVGW</td>\n",
       "      <td>HLA-B*44:03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>APIWPYEILY</td>\n",
       "      <td>HLA-B*35:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>VPYVNVAPSSSWTQH</td>\n",
       "      <td>H2-b class I</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>YSNQYQNSIDLSAS</td>\n",
       "      <td>H2-b class I</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LLCPAGINAV</td>\n",
       "      <td>HLA-A2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>FLFDGSPTYVL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LTILVALALFLLAAHASARQ</td>\n",
       "      <td>HLA-class II</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>FVFLRNFSL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ALENKRKQL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>QLAAGGKHL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>RTAGPSVGGV</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GGFVPNMLSV</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FLFLRNFSL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>FPPSPLFFFL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>SYVPSAEQIL</td>\n",
       "      <td>H2-Kd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GASPAVSSL</td>\n",
       "      <td>HLA-class I</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>FLPSPLFFFL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>CMTWNQMNL</td>\n",
       "      <td>HLA-A*24:02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>CYTWNQMNL</td>\n",
       "      <td>HLA-A*24:02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>IPGVAYTSPEVAWVG</td>\n",
       "      <td>H2-d class I</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>EEASSTRGNLDVAKLNGDWF</td>\n",
       "      <td>HLA-class II</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77940</th>\n",
       "      <td>PFVKIVEHHTLMTTH</td>\n",
       "      <td>H2-IEd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77941</th>\n",
       "      <td>TYWCYITEL</td>\n",
       "      <td>H2-Kd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77942</th>\n",
       "      <td>SYNSVKEII</td>\n",
       "      <td>H2-Kd</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77943</th>\n",
       "      <td>AVKNYCSKL</td>\n",
       "      <td>H2-Db</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77944</th>\n",
       "      <td>SNPTYSVM</td>\n",
       "      <td>H2-Kb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77945</th>\n",
       "      <td>MGVANLDNL</td>\n",
       "      <td>H2-Db</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77946</th>\n",
       "      <td>SIINFEHL</td>\n",
       "      <td>H2-Kb</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77947</th>\n",
       "      <td>IVLGLIATA</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77948</th>\n",
       "      <td>KVLGLWATV</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77949</th>\n",
       "      <td>LQLCCLATA</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77950</th>\n",
       "      <td>RGTPMVITV</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77951</th>\n",
       "      <td>KLNPMLAKA</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77952</th>\n",
       "      <td>RDVPMLITT</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77953</th>\n",
       "      <td>LALPMPATA</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77954</th>\n",
       "      <td>NDFCCVATV</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77955</th>\n",
       "      <td>RINAILATA</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77956</th>\n",
       "      <td>GGNGMLATI</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77957</th>\n",
       "      <td>KDLVLLATI</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77958</th>\n",
       "      <td>RLNTVLATA</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77959</th>\n",
       "      <td>MGLPGVATV</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77960</th>\n",
       "      <td>LVLPILITI</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77961</th>\n",
       "      <td>RVNRLIIWV</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77962</th>\n",
       "      <td>SGDGLVATG</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77963</th>\n",
       "      <td>MGNGCLRIV</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77964</th>\n",
       "      <td>NGVRVLATA</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77965</th>\n",
       "      <td>MINPLVITT</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77966</th>\n",
       "      <td>NIVCPLCTL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77967</th>\n",
       "      <td>ITNCLLSTA</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77968</th>\n",
       "      <td>KAVYNLATM</td>\n",
       "      <td>H2-Db</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77969</th>\n",
       "      <td>KAVCNFATM</td>\n",
       "      <td>H2-Db</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44524 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Description   Allele Name  bin\n",
       "0                 KLEDLERDL   HLA-A*02:01    1\n",
       "1           IEQQADNMITEMLQK        H2-IAk    1\n",
       "2      SGEGWPYIACRTSVVGRAWE        H2-IAb    1\n",
       "3                  TRVAFAGL         H2-Kb    1\n",
       "4                 AVAKAGKPL   HLA-E*01:01    1\n",
       "5                 AMLQDIATL   HLA-E*01:01    1\n",
       "6                 KMLRGVNVL   HLA-E*01:01    1\n",
       "7                 AAVEELKAL   HLA-E*01:01    0\n",
       "8                 VEGEALATL   HLA-E*01:01    0\n",
       "9                AEWPTFNVGW   HLA-B*44:03    1\n",
       "10               APIWPYEILY   HLA-B*35:01    1\n",
       "11          VPYVNVAPSSSWTQH  H2-b class I    1\n",
       "12           YSNQYQNSIDLSAS  H2-b class I    1\n",
       "13               LLCPAGINAV        HLA-A2    0\n",
       "14              FLFDGSPTYVL   HLA-A*02:01    1\n",
       "15     LTILVALALFLLAAHASARQ  HLA-class II    0\n",
       "16                FVFLRNFSL   HLA-A*02:01    1\n",
       "17                ALENKRKQL   HLA-A*02:01    0\n",
       "18                QLAAGGKHL   HLA-A*02:01    0\n",
       "19               RTAGPSVGGV   HLA-A*02:01    0\n",
       "20               GGFVPNMLSV   HLA-A*02:01    0\n",
       "21                FLFLRNFSL   HLA-A*02:01    1\n",
       "22               FPPSPLFFFL   HLA-A*02:01    1\n",
       "23               SYVPSAEQIL         H2-Kd    1\n",
       "24                GASPAVSSL   HLA-class I    1\n",
       "25               FLPSPLFFFL   HLA-A*02:01    1\n",
       "26                CMTWNQMNL   HLA-A*24:02    1\n",
       "27                CYTWNQMNL   HLA-A*24:02    1\n",
       "28          IPGVAYTSPEVAWVG  H2-d class I    1\n",
       "29     EEASSTRGNLDVAKLNGDWF  HLA-class II    1\n",
       "...                     ...           ...  ...\n",
       "77940       PFVKIVEHHTLMTTH        H2-IEd    1\n",
       "77941             TYWCYITEL         H2-Kd    1\n",
       "77942             SYNSVKEII         H2-Kd    1\n",
       "77943             AVKNYCSKL         H2-Db    1\n",
       "77944              SNPTYSVM         H2-Kb    1\n",
       "77945             MGVANLDNL         H2-Db    1\n",
       "77946              SIINFEHL         H2-Kb    1\n",
       "77947             IVLGLIATA   HLA-A*02:01    1\n",
       "77948             KVLGLWATV   HLA-A*02:01    1\n",
       "77949             LQLCCLATA   HLA-A*02:01    1\n",
       "77950             RGTPMVITV   HLA-A*02:01    1\n",
       "77951             KLNPMLAKA   HLA-A*02:01    1\n",
       "77952             RDVPMLITT   HLA-A*02:01    1\n",
       "77953             LALPMPATA   HLA-A*02:01    1\n",
       "77954             NDFCCVATV   HLA-A*02:01    1\n",
       "77955             RINAILATA   HLA-A*02:01    1\n",
       "77956             GGNGMLATI   HLA-A*02:01    1\n",
       "77957             KDLVLLATI   HLA-A*02:01    1\n",
       "77958             RLNTVLATA   HLA-A*02:01    1\n",
       "77959             MGLPGVATV   HLA-A*02:01    1\n",
       "77960             LVLPILITI   HLA-A*02:01    1\n",
       "77961             RVNRLIIWV   HLA-A*02:01    1\n",
       "77962             SGDGLVATG   HLA-A*02:01    1\n",
       "77963             MGNGCLRIV   HLA-A*02:01    1\n",
       "77964             NGVRVLATA   HLA-A*02:01    1\n",
       "77965             MINPLVITT   HLA-A*02:01    1\n",
       "77966             NIVCPLCTL   HLA-A*02:01    1\n",
       "77967             ITNCLLSTA   HLA-A*02:01    1\n",
       "77968             KAVYNLATM         H2-Db    1\n",
       "77969             KAVCNFATM         H2-Db    1\n",
       "\n",
       "[44524 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcell_clean1=tcell_comp[tcell_comp['Allele Name'].notnull()]\n",
    "tcell_clean2=tcell_clean1[tcell_clean1['Description'].notnull()]\n",
    "tcell_clean=tcell_clean2[tcell_clean2['bin'].notnull()]\n",
    "\n",
    "print tcell_clean.shape\n",
    "tcell_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15243\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Allele Name</th>\n",
       "      <th>bin</th>\n",
       "      <th>Allel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KLEDLERDL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IEQQADNMITEMLQK</td>\n",
       "      <td>H2-IAk</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGEGWPYIACRTSVVGRAWE</td>\n",
       "      <td>H2-IAb</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRVAFAGL</td>\n",
       "      <td>H2-Kb</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVAKAGKPL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>1</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AMLQDIATL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>1</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KMLRGVNVL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>1</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAVEELKAL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>0</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VEGEALATL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>0</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AEWPTFNVGW</td>\n",
       "      <td>HLA-B*44:03</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Description  Allele Name  bin  Allel\n",
       "0             KLEDLERDL  HLA-A*02:01    1     91\n",
       "1       IEQQADNMITEMLQK       H2-IAk    1     49\n",
       "2  SGEGWPYIACRTSVVGRAWE       H2-IAb    1     46\n",
       "3              TRVAFAGL        H2-Kb    1     62\n",
       "4             AVAKAGKPL  HLA-E*01:01    1    328\n",
       "5             AMLQDIATL  HLA-E*01:01    1    328\n",
       "6             KMLRGVNVL  HLA-E*01:01    1    328\n",
       "7             AAVEELKAL  HLA-E*01:01    0    328\n",
       "8             VEGEALATL  HLA-E*01:01    0    328\n",
       "9            AEWPTFNVGW  HLA-B*44:03    1    154"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print np.sum(tcell_clean['bin'])\n",
    "tcell_clean['Allel']=pd.Categorical.from_array(tcell_clean['Allele Name']).codes\n",
    "tcell_clean.head(10)\n",
    "#plt.hist(np.histogram(tcell_clean['Allele Name'],bins=50))\n",
    "#plt.show()\n",
    "#print np.sum(np.asarray(tcell_all['MHC Allele ID']) < 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Description', u'Allele Name', u'bin', u'Allel'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# scaling factors\n",
    "print tcell_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_ascii(letter):\n",
    "    if pd.isnull(letter):\n",
    "        return 0\n",
    "    else:\n",
    "        return ord(letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44524, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Int64Index([    0,     1,     2,     3,     4,     5,     6,     7,     8,\n",
       "                9,\n",
       "            ...\n",
       "            77960, 77961, 77962, 77963, 77964, 77965, 77966, 77967, 77968,\n",
       "            77969],\n",
       "           dtype='int64', length=44524)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print tcell_clean.shape\n",
    "tcell_clean.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n",
      "1\n",
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[73, 69, 81, 81, 65, 68, 78, 77, 73, 84, 69, 77, 76, 81, 75]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leng_seq=len(tcell_clean.Description.get(1))\n",
    "print tcell_clean.Allel.get(0)\n",
    "print tcell_clean.bin.get(0)\n",
    "print leng_seq\n",
    "map(to_ascii,tcell_clean.Description.get(1))[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44524\n"
     ]
    }
   ],
   "source": [
    "number_data=tcell_clean.shape[0]\n",
    "number_feature=51\n",
    "tcell_data=np.zeros([number_data,number_feature+1])\n",
    "print number_data\n",
    "j=0\n",
    "for i in tcell_clean.index:\n",
    "    tcell_data[j,0]=tcell_clean.Allel.get(i)\n",
    "    tcell_data[j,number_feature]=tcell_clean.bin.get(i)\n",
    "    leng_seq=len(tcell_clean.Description.get(i))\n",
    "    if leng_seq <=number_feature-1:\n",
    "        tcell_data[j,1:leng_seq+1]=map(to_ascii,tcell_clean.Description.get(i))\n",
    "    else:\n",
    "        tcell_data[j,1:number_feature]=map(to_ascii,tcell_clean.Description.get(i))[:number_feature-1]\n",
    "    j=j+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 62.  84.  82.  86.  65.  70.  65.  71.  76.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.   1.]\n"
     ]
    }
   ],
   "source": [
    "print tcell_data[3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Allele Name</th>\n",
       "      <th>bin</th>\n",
       "      <th>Allel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KLEDLERDL</td>\n",
       "      <td>HLA-A*02:01</td>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IEQQADNMITEMLQK</td>\n",
       "      <td>H2-IAk</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SGEGWPYIACRTSVVGRAWE</td>\n",
       "      <td>H2-IAb</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRVAFAGL</td>\n",
       "      <td>H2-Kb</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AVAKAGKPL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>1</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AMLQDIATL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>1</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KMLRGVNVL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>1</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAVEELKAL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>0</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>VEGEALATL</td>\n",
       "      <td>HLA-E*01:01</td>\n",
       "      <td>0</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AEWPTFNVGW</td>\n",
       "      <td>HLA-B*44:03</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Description  Allele Name  bin  Allel\n",
       "0             KLEDLERDL  HLA-A*02:01    1     91\n",
       "1       IEQQADNMITEMLQK       H2-IAk    1     49\n",
       "2  SGEGWPYIACRTSVVGRAWE       H2-IAb    1     46\n",
       "3              TRVAFAGL        H2-Kb    1     62\n",
       "4             AVAKAGKPL  HLA-E*01:01    1    328\n",
       "5             AMLQDIATL  HLA-E*01:01    1    328\n",
       "6             KMLRGVNVL  HLA-E*01:01    1    328\n",
       "7             AAVEELKAL  HLA-E*01:01    0    328\n",
       "8             VEGEALATL  HLA-E*01:01    0    328\n",
       "9            AEWPTFNVGW  HLA-B*44:03    1    154"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcell_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 332.,   69.,   77.,   69.,   69.,   67.,   83.,   81.,   72.,\n",
       "          76.,   80.,   89.,   73.,   69.,   81.,   71.,   77.,   77.,\n",
       "          76.,   65.,   69.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    1.],\n",
       "       [ 332.,   87.,   86.,   65.,   70.,   80.,   76.,   65.,   76.,\n",
       "          76.,   83.,   71.,   86.,   73.,   89.,   86.,   73.,   76.,\n",
       "          82.,   75.,   82.,   69.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
       "           0.,    0.,    0.,    0.,    0.,    0.,    0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.shuffle(tcell_data)\n",
    "tcell_data[:2,:]\n",
    "#tcell_shuf.dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 50) (30000, 2) (7000, 50) (7000, 2) (7524, 50) (7524, 2)\n"
     ]
    }
   ],
   "source": [
    "tcell_array=np.asarray(tcell_data)\n",
    "num_total=tcell_array.shape[0]\n",
    "train_num=30000\n",
    "valid_num=7000\n",
    "test_num=num_total-train_num-valid_num\n",
    "train_set=tcell_array[:train_num,1:number_feature]\n",
    "#train_lable=tcell_array[:train_num,number_feature]\n",
    "train_lable=(np.arange(2)==tcell_array[:train_num,number_feature][:,None]).astype(np.float32)\n",
    "\n",
    "valid_set=tcell_array[train_num:train_num+valid_num,1:number_feature]\n",
    "#valid_lable=tcell_array[]\n",
    "valid_lable=(np.arange(2)==tcell_array[train_num:train_num+valid_num,number_feature][:,None]).astype(np.float32)\n",
    "\n",
    "test_set=tcell_array[train_num+valid_num:,1:number_feature]\n",
    "#test_lable=tcell_array[train_num+valid_num:,number_feature]\n",
    "test_lable=(np.arange(2)==tcell_array[train_num+valid_num:,number_feature][:,None]).astype(np.float32)\n",
    "\n",
    "print train_set.shape, train_lable.shape, valid_set.shape, valid_lable.shape, test_set.shape, test_lable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_feature1=number_feature-1\n",
    "batch_size=1000\n",
    "hidden_size=20\n",
    "num_lable=2\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "    train_set_batch=tf.placeholder(tf.float32,shape=(batch_size,number_feature1))\n",
    "    train_lable_batch=tf.placeholder(tf.float32,shape=(batch_size,num_lable))\n",
    "    \n",
    "    valid_set_tf=tf.constant(valid_set.astype(np.float32))\n",
    "    test_set_tf=tf.constant(test_set.astype(np.float32))\n",
    "    \n",
    "    weights1=tf.Variable(tf.truncated_normal([number_feature1,hidden_size]))\n",
    "    bias1=tf.Variable(tf.zeros([hidden_size]))\n",
    "    \n",
    "    weights2=tf.Variable(tf.truncated_normal([hidden_size,num_lable]))\n",
    "    bias2=tf.Variable(tf.zeros([num_lable]))\n",
    "    \n",
    "    logits1=tf.matmul(train_set_batch,weights1)+bias1\n",
    "    act1=tf.nn.relu(logits1)\n",
    "    logits=tf.matmul(act1,weights2)+bias2\n",
    "\n",
    "    loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,train_lable_batch))\n",
    "    \n",
    "    optimizer=tf.train.GradientDescentOptimizer(0.000005).minimize(loss)\n",
    "    \n",
    "    train_pred=tf.nn.softmax(logits)\n",
    "    valid_pred=tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(valid_set_tf,weights1)+bias1),weights2)+bias2)\n",
    "    test_pred=tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(test_set_tf,weights1)+bias1),weights2)+bias2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 79.816971\n",
      "Minibatch accuracy: 47.2%\n",
      "Validation accuracy: 45.4%\n",
      "Minibatch loss at step 20: 84.908600\n",
      "Minibatch accuracy: 45.6%\n",
      "Validation accuracy: 45.8%\n",
      "Minibatch loss at step 40: 74.933075\n",
      "Minibatch accuracy: 49.8%\n",
      "Validation accuracy: 45.9%\n",
      "Minibatch loss at step 60: 74.586334\n",
      "Minibatch accuracy: 47.7%\n",
      "Validation accuracy: 45.8%\n",
      "Minibatch loss at step 80: 79.630157\n",
      "Minibatch accuracy: 45.8%\n",
      "Validation accuracy: 46.0%\n",
      "Minibatch loss at step 100: 69.818306\n",
      "Minibatch accuracy: 50.3%\n",
      "Validation accuracy: 45.9%\n",
      "Minibatch loss at step 120: 69.827065\n",
      "Minibatch accuracy: 46.4%\n",
      "Validation accuracy: 46.0%\n",
      "Minibatch loss at step 140: 74.722778\n",
      "Minibatch accuracy: 45.2%\n",
      "Validation accuracy: 46.0%\n",
      "Minibatch loss at step 160: 65.137520\n",
      "Minibatch accuracy: 50.2%\n",
      "Validation accuracy: 45.9%\n",
      "Minibatch loss at step 180: 65.539597\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 45.8%\n",
      "Minibatch loss at step 200: 70.250534\n",
      "Minibatch accuracy: 45.0%\n",
      "Validation accuracy: 45.7%\n",
      "Minibatch loss at step 220: 60.898735\n",
      "Minibatch accuracy: 50.6%\n",
      "Validation accuracy: 45.4%\n",
      "Minibatch loss at step 240: 61.734726\n",
      "Minibatch accuracy: 46.8%\n",
      "Validation accuracy: 45.2%\n",
      "Minibatch loss at step 260: 66.274536\n",
      "Minibatch accuracy: 43.5%\n",
      "Validation accuracy: 45.4%\n",
      "Minibatch loss at step 280: 57.163513\n",
      "Minibatch accuracy: 50.5%\n",
      "Validation accuracy: 45.5%\n",
      "Minibatch loss at step 300: 58.387562\n",
      "Minibatch accuracy: 46.0%\n",
      "Validation accuracy: 45.6%\n",
      "Minibatch loss at step 320: 62.789024\n",
      "Minibatch accuracy: 43.6%\n",
      "Validation accuracy: 45.5%\n",
      "Minibatch loss at step 340: 53.852180\n",
      "Minibatch accuracy: 51.1%\n",
      "Validation accuracy: 45.1%\n",
      "Minibatch loss at step 360: 55.420727\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 45.0%\n",
      "Minibatch loss at step 380: 59.693253\n",
      "Minibatch accuracy: 44.2%\n",
      "Validation accuracy: 45.1%\n",
      "Minibatch loss at step 400: 50.893383\n",
      "Minibatch accuracy: 51.2%\n",
      "Validation accuracy: 45.1%\n",
      "Minibatch loss at step 420: 52.801128\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 45.1%\n",
      "Minibatch loss at step 440: 56.912846\n",
      "Minibatch accuracy: 44.8%\n",
      "Validation accuracy: 45.0%\n",
      "Minibatch loss at step 460: 48.238312\n",
      "Minibatch accuracy: 50.8%\n",
      "Validation accuracy: 45.0%\n",
      "Minibatch loss at step 480: 50.471851\n",
      "Minibatch accuracy: 47.0%\n",
      "Validation accuracy: 45.0%\n",
      "Minibatch loss at step 500: 54.425499\n",
      "Minibatch accuracy: 46.0%\n",
      "Validation accuracy: 45.0%\n",
      "Minibatch loss at step 520: 45.778316\n",
      "Minibatch accuracy: 51.0%\n",
      "Validation accuracy: 45.0%\n",
      "Minibatch loss at step 540: 48.357193\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 45.0%\n",
      "Minibatch loss at step 560: 52.136913\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 45.1%\n",
      "Minibatch loss at step 580: 43.533726\n",
      "Minibatch accuracy: 51.2%\n",
      "Validation accuracy: 45.0%\n",
      "Minibatch loss at step 600: 46.395828\n",
      "Minibatch accuracy: 47.4%\n",
      "Validation accuracy: 45.1%\n",
      "Minibatch loss at step 620: 49.991493\n",
      "Minibatch accuracy: 46.8%\n",
      "Validation accuracy: 45.3%\n",
      "Minibatch loss at step 640: 41.458225\n",
      "Minibatch accuracy: 50.4%\n",
      "Validation accuracy: 45.4%\n",
      "Minibatch loss at step 660: 44.623722\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 45.6%\n",
      "Minibatch loss at step 680: 47.991531\n",
      "Minibatch accuracy: 46.5%\n",
      "Validation accuracy: 45.7%\n",
      "Minibatch loss at step 700: 39.513680\n",
      "Minibatch accuracy: 50.1%\n",
      "Validation accuracy: 46.0%\n",
      "Minibatch loss at step 720: 43.026070\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 46.1%\n",
      "Minibatch loss at step 740: 46.126102\n",
      "Minibatch accuracy: 47.4%\n",
      "Validation accuracy: 46.1%\n",
      "Minibatch loss at step 760: 37.717209\n",
      "Minibatch accuracy: 50.3%\n",
      "Validation accuracy: 46.2%\n",
      "Minibatch loss at step 780: 41.542130\n",
      "Minibatch accuracy: 48.6%\n",
      "Validation accuracy: 46.4%\n",
      "Minibatch loss at step 800: 44.424320\n",
      "Minibatch accuracy: 48.2%\n",
      "Validation accuracy: 46.5%\n",
      "Minibatch loss at step 820: 36.095009\n",
      "Minibatch accuracy: 50.2%\n",
      "Validation accuracy: 46.5%\n",
      "Minibatch loss at step 840: 40.190659\n",
      "Minibatch accuracy: 49.3%\n",
      "Validation accuracy: 46.6%\n",
      "Minibatch loss at step 860: 42.848515\n",
      "Minibatch accuracy: 48.6%\n",
      "Validation accuracy: 46.6%\n",
      "Minibatch loss at step 880: 34.625885\n",
      "Minibatch accuracy: 50.5%\n",
      "Validation accuracy: 46.8%\n",
      "Minibatch loss at step 900: 38.954384\n",
      "Minibatch accuracy: 48.6%\n",
      "Validation accuracy: 47.1%\n",
      "Minibatch loss at step 920: 41.395096\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 47.4%\n",
      "Minibatch loss at step 940: 33.302258\n",
      "Minibatch accuracy: 50.7%\n",
      "Validation accuracy: 47.5%\n",
      "Minibatch loss at step 960: 37.838211\n",
      "Minibatch accuracy: 48.0%\n",
      "Validation accuracy: 47.7%\n",
      "Minibatch loss at step 980: 40.068733\n",
      "Minibatch accuracy: 48.2%\n",
      "Validation accuracy: 48.1%\n",
      "Minibatch loss at step 1000: 32.136589\n",
      "Minibatch accuracy: 51.3%\n",
      "Validation accuracy: 48.3%\n",
      "Minibatch loss at step 1020: 36.849968\n",
      "Minibatch accuracy: 48.1%\n",
      "Validation accuracy: 48.6%\n",
      "Minibatch loss at step 1040: 38.883484\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 49.0%\n",
      "Minibatch loss at step 1060: 31.136057\n",
      "Minibatch accuracy: 51.4%\n",
      "Validation accuracy: 49.1%\n",
      "Minibatch loss at step 1080: 35.984772\n",
      "Minibatch accuracy: 48.7%\n",
      "Validation accuracy: 49.0%\n",
      "Minibatch loss at step 1100: 37.839775\n",
      "Minibatch accuracy: 47.7%\n",
      "Validation accuracy: 49.0%\n",
      "Minibatch loss at step 1120: 30.291973\n",
      "Minibatch accuracy: 50.6%\n",
      "Validation accuracy: 49.3%\n",
      "Minibatch loss at step 1140: 35.254219\n",
      "Minibatch accuracy: 49.4%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 1160: 36.948364\n",
      "Minibatch accuracy: 47.2%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 1180: 29.598907\n",
      "Minibatch accuracy: 51.2%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 1200: 34.625851\n",
      "Minibatch accuracy: 49.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 1220: 36.194328\n",
      "Minibatch accuracy: 47.3%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 1240: 29.038090\n",
      "Minibatch accuracy: 51.9%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 1260: 34.073502\n",
      "Minibatch accuracy: 49.4%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 1280: 35.575230\n",
      "Minibatch accuracy: 47.3%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 1300: 28.592506\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy: 50.4%\n",
      "Minibatch loss at step 1320: 33.593628\n",
      "Minibatch accuracy: 49.4%\n",
      "Validation accuracy: 50.3%\n",
      "Minibatch loss at step 1340: 35.062790\n",
      "Minibatch accuracy: 47.7%\n",
      "Validation accuracy: 50.4%\n",
      "Minibatch loss at step 1360: 28.229284\n",
      "Minibatch accuracy: 53.5%\n",
      "Validation accuracy: 50.4%\n",
      "Minibatch loss at step 1380: 33.174896\n",
      "Minibatch accuracy: 49.5%\n",
      "Validation accuracy: 50.5%\n",
      "Minibatch loss at step 1400: 34.637264\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 50.6%\n",
      "Minibatch loss at step 1420: 27.922386\n",
      "Minibatch accuracy: 53.6%\n",
      "Validation accuracy: 50.6%\n",
      "Minibatch loss at step 1440: 32.815571\n",
      "Minibatch accuracy: 48.3%\n",
      "Validation accuracy: 50.6%\n",
      "Minibatch loss at step 1460: 34.277431\n",
      "Minibatch accuracy: 47.5%\n",
      "Validation accuracy: 50.5%\n",
      "Minibatch loss at step 1480: 27.659306\n",
      "Minibatch accuracy: 53.5%\n",
      "Validation accuracy: 50.6%\n",
      "Minibatch loss at step 1500: 32.502499\n",
      "Minibatch accuracy: 48.1%\n",
      "Validation accuracy: 50.5%\n",
      "Minibatch loss at step 1520: 33.962273\n",
      "Minibatch accuracy: 47.2%\n",
      "Validation accuracy: 50.4%\n",
      "Minibatch loss at step 1540: 27.429670\n",
      "Minibatch accuracy: 53.4%\n",
      "Validation accuracy: 50.4%\n",
      "Minibatch loss at step 1560: 32.218964\n",
      "Minibatch accuracy: 48.0%\n",
      "Validation accuracy: 50.4%\n",
      "Minibatch loss at step 1580: 33.677380\n",
      "Minibatch accuracy: 46.2%\n",
      "Validation accuracy: 50.3%\n",
      "Minibatch loss at step 1600: 27.223162\n",
      "Minibatch accuracy: 53.3%\n",
      "Validation accuracy: 50.3%\n",
      "Minibatch loss at step 1620: 31.956816\n",
      "Minibatch accuracy: 48.1%\n",
      "Validation accuracy: 50.4%\n",
      "Minibatch loss at step 1640: 33.417225\n",
      "Minibatch accuracy: 45.8%\n",
      "Validation accuracy: 50.4%\n",
      "Minibatch loss at step 1660: 27.030848\n",
      "Minibatch accuracy: 53.2%\n",
      "Validation accuracy: 50.3%\n",
      "Minibatch loss at step 1680: 31.711645\n",
      "Minibatch accuracy: 48.9%\n",
      "Validation accuracy: 50.3%\n",
      "Minibatch loss at step 1700: 33.173275\n",
      "Minibatch accuracy: 45.7%\n",
      "Validation accuracy: 50.3%\n",
      "Minibatch loss at step 1720: 26.850096\n",
      "Minibatch accuracy: 52.8%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 1740: 31.478315\n",
      "Minibatch accuracy: 49.0%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 1760: 32.943008\n",
      "Minibatch accuracy: 45.6%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 1780: 26.676292\n",
      "Minibatch accuracy: 52.8%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 1800: 31.257694\n",
      "Minibatch accuracy: 48.3%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 1820: 32.725330\n",
      "Minibatch accuracy: 45.7%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 1840: 26.507757\n",
      "Minibatch accuracy: 52.7%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 1860: 31.044697\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 1880: 32.517197\n",
      "Minibatch accuracy: 45.6%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 1900: 26.343716\n",
      "Minibatch accuracy: 52.7%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 1920: 30.839245\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 1940: 32.318771\n",
      "Minibatch accuracy: 45.7%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 1960: 26.183615\n",
      "Minibatch accuracy: 52.7%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 1980: 30.642021\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 2000: 32.127380\n",
      "Minibatch accuracy: 46.0%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 2020: 26.026981\n",
      "Minibatch accuracy: 52.8%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 2040: 30.451336\n",
      "Minibatch accuracy: 48.2%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2060: 31.943848\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 2080: 25.873503\n",
      "Minibatch accuracy: 52.4%\n",
      "Validation accuracy: 50.3%\n",
      "Minibatch loss at step 2100: 30.268007\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 2120: 31.769367\n",
      "Minibatch accuracy: 47.0%\n",
      "Validation accuracy: 50.3%\n",
      "Minibatch loss at step 2140: 25.721382\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 50.3%\n",
      "Minibatch loss at step 2160: 30.089218\n",
      "Minibatch accuracy: 48.3%\n",
      "Validation accuracy: 50.3%\n",
      "Minibatch loss at step 2180: 31.604330\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 50.3%\n",
      "Minibatch loss at step 2200: 25.572901\n",
      "Minibatch accuracy: 52.8%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 2220: 29.914484\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 2240: 31.442755\n",
      "Minibatch accuracy: 46.7%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2260: 25.427021\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2280: 29.744991\n",
      "Minibatch accuracy: 48.3%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2300: 31.283566\n",
      "Minibatch accuracy: 46.8%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2320: 25.283577\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2340: 29.579691\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2360: 31.129265\n",
      "Minibatch accuracy: 46.5%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2380: 25.141527\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2400: 29.417616\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2420: 30.978422\n",
      "Minibatch accuracy: 46.7%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2440: 25.000780\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2460: 29.259016\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2480: 30.831053\n",
      "Minibatch accuracy: 47.0%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 2500: 24.861300\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2520: 29.103970\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2540: 30.688658\n",
      "Minibatch accuracy: 46.7%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 2560: 24.723749\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 2580: 28.951410\n",
      "Minibatch accuracy: 48.7%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2600: 30.549368\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2620: 24.588673\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2640: 28.801029\n",
      "Minibatch accuracy: 48.8%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2660: 30.410753\n",
      "Minibatch accuracy: 46.4%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2680: 24.456043\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2700: 28.652966\n",
      "Minibatch accuracy: 48.8%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2720: 30.274067\n",
      "Minibatch accuracy: 46.4%\n",
      "Validation accuracy: 50.2%\n",
      "Minibatch loss at step 2740: 24.326273\n",
      "Minibatch accuracy: 52.4%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2760: 28.508099\n",
      "Minibatch accuracy: 48.7%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2780: 30.140446\n",
      "Minibatch accuracy: 46.2%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 2800: 24.199129\n",
      "Minibatch accuracy: 52.3%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2820: 28.366945\n",
      "Minibatch accuracy: 48.7%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2840: 30.008913\n",
      "Minibatch accuracy: 46.2%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2860: 24.073084\n",
      "Minibatch accuracy: 52.3%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2880: 28.228235\n",
      "Minibatch accuracy: 48.8%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2900: 29.878658\n",
      "Minibatch accuracy: 46.2%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2920: 23.948278\n",
      "Minibatch accuracy: 52.3%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2940: 28.091616\n",
      "Minibatch accuracy: 48.9%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 2960: 29.750216\n",
      "Minibatch accuracy: 46.4%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 2980: 23.825016\n",
      "Minibatch accuracy: 52.1%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 3000: 27.957798\n",
      "Minibatch accuracy: 49.2%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 3020: 29.623121\n",
      "Minibatch accuracy: 46.3%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 3040: 23.703430\n",
      "Minibatch accuracy: 51.8%\n",
      "Validation accuracy: 50.1%\n",
      "Minibatch loss at step 3060: 27.825294\n",
      "Minibatch accuracy: 48.9%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 3080: 29.498184\n",
      "Minibatch accuracy: 46.4%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 3100: 23.584370\n",
      "Minibatch accuracy: 51.5%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 3120: 27.694231\n",
      "Minibatch accuracy: 48.7%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 3140: 29.375488\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 3160: 23.467068\n",
      "Minibatch accuracy: 51.5%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 3180: 27.564341\n",
      "Minibatch accuracy: 48.7%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 3200: 29.254164\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 3220: 23.351774\n",
      "Minibatch accuracy: 51.8%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 3240: 27.435509\n",
      "Minibatch accuracy: 48.7%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 3260: 29.134535\n",
      "Minibatch accuracy: 46.0%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 3280: 23.240044\n",
      "Minibatch accuracy: 51.8%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 3300: 27.307884\n",
      "Minibatch accuracy: 48.7%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 3320: 29.016874\n",
      "Minibatch accuracy: 45.7%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 3340: 23.130808\n",
      "Minibatch accuracy: 51.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 3360: 27.181543\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3380: 28.900635\n",
      "Minibatch accuracy: 45.8%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 3400: 23.023470\n",
      "Minibatch accuracy: 51.7%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 3420: 27.056778\n",
      "Minibatch accuracy: 49.0%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3440: 28.785866\n",
      "Minibatch accuracy: 45.8%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 3460: 22.918274\n",
      "Minibatch accuracy: 51.8%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 3480: 26.933729\n",
      "Minibatch accuracy: 49.2%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3500: 28.671995\n",
      "Minibatch accuracy: 45.7%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 3520: 22.814352\n",
      "Minibatch accuracy: 51.9%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 3540: 26.812309\n",
      "Minibatch accuracy: 49.2%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3560: 28.559538\n",
      "Minibatch accuracy: 45.7%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3580: 22.711250\n",
      "Minibatch accuracy: 52.1%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 3600: 26.692490\n",
      "Minibatch accuracy: 49.2%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3620: 28.448547\n",
      "Minibatch accuracy: 45.9%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 3640: 22.609045\n",
      "Minibatch accuracy: 52.1%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3660: 26.573593\n",
      "Minibatch accuracy: 49.5%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3680: 28.337784\n",
      "Minibatch accuracy: 46.0%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3700: 22.507921\n",
      "Minibatch accuracy: 52.4%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3720: 26.455191\n",
      "Minibatch accuracy: 49.7%\n",
      "Validation accuracy: 49.4%\n",
      "Minibatch loss at step 3740: 28.227839\n",
      "Minibatch accuracy: 46.0%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3760: 22.408245\n",
      "Minibatch accuracy: 52.2%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3780: 26.337917\n",
      "Minibatch accuracy: 49.5%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3800: 28.118868\n",
      "Minibatch accuracy: 45.7%\n",
      "Validation accuracy: 49.4%\n",
      "Minibatch loss at step 3820: 22.309978\n",
      "Minibatch accuracy: 52.2%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3840: 26.222031\n",
      "Minibatch accuracy: 49.1%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3860: 28.010288\n",
      "Minibatch accuracy: 45.7%\n",
      "Validation accuracy: 49.4%\n",
      "Minibatch loss at step 3880: 22.213963\n",
      "Minibatch accuracy: 52.0%\n",
      "Validation accuracy: 49.4%\n",
      "Minibatch loss at step 3900: 26.107857\n",
      "Minibatch accuracy: 48.8%\n",
      "Validation accuracy: 49.4%\n",
      "Minibatch loss at step 3920: 27.902853\n",
      "Minibatch accuracy: 45.7%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 3940: 22.120300\n",
      "Minibatch accuracy: 51.7%\n",
      "Validation accuracy: 49.4%\n",
      "Minibatch loss at step 3960: 25.995543\n",
      "Minibatch accuracy: 48.6%\n",
      "Validation accuracy: 49.4%\n",
      "Minibatch loss at step 3980: 27.797405\n",
      "Minibatch accuracy: 45.4%\n",
      "Validation accuracy: 49.4%\n",
      "Minibatch loss at step 4000: 22.028156\n",
      "Minibatch accuracy: 51.7%\n",
      "Validation accuracy: 49.4%\n",
      "Minibatch loss at step 4020: 25.885117\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.4%\n",
      "Minibatch loss at step 4040: 27.693062\n",
      "Minibatch accuracy: 45.4%\n",
      "Validation accuracy: 49.4%\n",
      "Minibatch loss at step 4060: 21.937023\n",
      "Minibatch accuracy: 51.8%\n",
      "Validation accuracy: 49.4%\n",
      "Minibatch loss at step 4080: 25.776443\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.4%\n",
      "Minibatch loss at step 4100: 27.590563\n",
      "Minibatch accuracy: 45.4%\n",
      "Validation accuracy: 49.4%\n",
      "Minibatch loss at step 4120: 21.846554\n",
      "Minibatch accuracy: 52.0%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 4140: 25.668901\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 4160: 27.489120\n",
      "Minibatch accuracy: 45.4%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 4180: 21.756590\n",
      "Minibatch accuracy: 51.7%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 4200: 25.562738\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 4220: 27.388727\n",
      "Minibatch accuracy: 45.4%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 4240: 21.667372\n",
      "Minibatch accuracy: 51.5%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 4260: 25.457626\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 4280: 27.289192\n",
      "Minibatch accuracy: 45.4%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 4300: 21.579391\n",
      "Minibatch accuracy: 51.5%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 4320: 25.352671\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 4340: 27.190363\n",
      "Minibatch accuracy: 45.2%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 4360: 21.492702\n",
      "Minibatch accuracy: 51.5%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 4380: 25.248867\n",
      "Minibatch accuracy: 48.8%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 4400: 27.092607\n",
      "Minibatch accuracy: 45.2%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 4420: 21.406929\n",
      "Minibatch accuracy: 51.5%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 4440: 25.145586\n",
      "Minibatch accuracy: 48.7%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 4460: 26.995754\n",
      "Minibatch accuracy: 45.2%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 4480: 21.322273\n",
      "Minibatch accuracy: 51.6%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 4500: 25.042809\n",
      "Minibatch accuracy: 48.9%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 4520: 26.899958\n",
      "Minibatch accuracy: 45.0%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 4540: 21.238562\n",
      "Minibatch accuracy: 51.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 4560: 24.941305\n",
      "Minibatch accuracy: 49.1%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 4580: 26.804886\n",
      "Minibatch accuracy: 45.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 4600: 21.155975\n",
      "Minibatch accuracy: 51.7%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 4620: 24.840834\n",
      "Minibatch accuracy: 49.1%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 4640: 26.709957\n",
      "Minibatch accuracy: 45.3%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 4660: 21.073999\n",
      "Minibatch accuracy: 51.7%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 4680: 24.741737\n",
      "Minibatch accuracy: 49.1%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 4700: 26.616119\n",
      "Minibatch accuracy: 45.3%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 4720: 20.992935\n",
      "Minibatch accuracy: 51.9%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 4740: 24.643555\n",
      "Minibatch accuracy: 49.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 4760: 26.523252\n",
      "Minibatch accuracy: 45.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 4780: 20.913633\n",
      "Minibatch accuracy: 51.9%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 4800: 24.546154\n",
      "Minibatch accuracy: 49.1%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 4820: 26.431257\n",
      "Minibatch accuracy: 45.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 4840: 20.835051\n",
      "Minibatch accuracy: 51.9%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 4860: 24.449726\n",
      "Minibatch accuracy: 49.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 4880: 26.340168\n",
      "Minibatch accuracy: 45.4%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 4900: 20.757122\n",
      "Minibatch accuracy: 51.7%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 4920: 24.354321\n",
      "Minibatch accuracy: 49.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 4940: 26.249706\n",
      "Minibatch accuracy: 45.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 4960: 20.679510\n",
      "Minibatch accuracy: 51.7%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 4980: 24.259354\n",
      "Minibatch accuracy: 49.1%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 5000: 26.159340\n",
      "Minibatch accuracy: 45.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5020: 20.603052\n",
      "Minibatch accuracy: 51.5%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5040: 24.164692\n",
      "Minibatch accuracy: 49.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5060: 26.068829\n",
      "Minibatch accuracy: 45.7%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5080: 20.527645\n",
      "Minibatch accuracy: 51.0%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5100: 24.070038\n",
      "Minibatch accuracy: 49.3%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5120: 25.978565\n",
      "Minibatch accuracy: 45.9%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5140: 20.453114\n",
      "Minibatch accuracy: 51.0%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5160: 23.976320\n",
      "Minibatch accuracy: 49.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5180: 25.888634\n",
      "Minibatch accuracy: 45.9%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5200: 20.379566\n",
      "Minibatch accuracy: 51.2%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5220: 23.883484\n",
      "Minibatch accuracy: 49.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5240: 25.798925\n",
      "Minibatch accuracy: 45.8%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5260: 20.306959\n",
      "Minibatch accuracy: 51.2%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5280: 23.791378\n",
      "Minibatch accuracy: 49.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5300: 25.709948\n",
      "Minibatch accuracy: 45.8%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5320: 20.235199\n",
      "Minibatch accuracy: 51.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5340: 23.699808\n",
      "Minibatch accuracy: 49.0%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 5360: 25.621645\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5380: 20.164188\n",
      "Minibatch accuracy: 51.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5400: 23.608723\n",
      "Minibatch accuracy: 48.7%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 5420: 25.533930\n",
      "Minibatch accuracy: 46.0%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5440: 20.092752\n",
      "Minibatch accuracy: 51.5%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5460: 23.518293\n",
      "Minibatch accuracy: 48.7%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 5480: 25.446974\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5500: 20.022335\n",
      "Minibatch accuracy: 51.7%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5520: 23.429237\n",
      "Minibatch accuracy: 48.6%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 5540: 25.360977\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5560: 19.952808\n",
      "Minibatch accuracy: 51.3%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5580: 23.341972\n",
      "Minibatch accuracy: 48.6%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 5600: 25.275900\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 5620: 19.884012\n",
      "Minibatch accuracy: 51.3%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 5640: 23.255957\n",
      "Minibatch accuracy: 48.3%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 5660: 25.190668\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 5680: 19.815613\n",
      "Minibatch accuracy: 51.3%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 5700: 23.170731\n",
      "Minibatch accuracy: 48.3%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 5720: 25.106073\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 5740: 19.747295\n",
      "Minibatch accuracy: 51.5%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 5760: 23.087536\n",
      "Minibatch accuracy: 48.0%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 5780: 25.022390\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 5800: 19.679382\n",
      "Minibatch accuracy: 51.5%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5820: 23.005453\n",
      "Minibatch accuracy: 48.2%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5840: 24.939081\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5860: 19.612028\n",
      "Minibatch accuracy: 51.5%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5880: 22.924498\n",
      "Minibatch accuracy: 48.2%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 5900: 24.856714\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5920: 19.544802\n",
      "Minibatch accuracy: 51.7%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5940: 22.844648\n",
      "Minibatch accuracy: 48.2%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 5960: 24.775280\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 5980: 19.477928\n",
      "Minibatch accuracy: 51.7%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6000: 22.765879\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6020: 24.694761\n",
      "Minibatch accuracy: 46.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6040: 19.411676\n",
      "Minibatch accuracy: 51.7%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6060: 22.687935\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6080: 24.614758\n",
      "Minibatch accuracy: 46.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6100: 19.345827\n",
      "Minibatch accuracy: 51.7%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6120: 22.609882\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6140: 24.535290\n",
      "Minibatch accuracy: 46.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6160: 19.280664\n",
      "Minibatch accuracy: 52.0%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6180: 22.532114\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6200: 24.456457\n",
      "Minibatch accuracy: 46.4%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6220: 19.216208\n",
      "Minibatch accuracy: 52.0%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6240: 22.455193\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6260: 24.379183\n",
      "Minibatch accuracy: 46.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6280: 19.152334\n",
      "Minibatch accuracy: 51.9%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6300: 22.379414\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 6320: 24.302984\n",
      "Minibatch accuracy: 46.4%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6340: 19.089050\n",
      "Minibatch accuracy: 51.9%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6360: 22.303846\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 6380: 24.227352\n",
      "Minibatch accuracy: 46.5%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6400: 19.025946\n",
      "Minibatch accuracy: 52.0%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6420: 22.228195\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6440: 24.152159\n",
      "Minibatch accuracy: 46.5%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6460: 18.962957\n",
      "Minibatch accuracy: 52.0%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6480: 22.152864\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6500: 24.077379\n",
      "Minibatch accuracy: 46.8%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6520: 18.900442\n",
      "Minibatch accuracy: 52.0%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6540: 22.076933\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6560: 24.003071\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6580: 18.838327\n",
      "Minibatch accuracy: 52.2%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 6600: 22.001047\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 6620: 23.929485\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 6640: 18.776693\n",
      "Minibatch accuracy: 52.2%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 6660: 21.925619\n",
      "Minibatch accuracy: 48.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6680: 23.856937\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 6700: 18.714895\n",
      "Minibatch accuracy: 52.2%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 6720: 21.850691\n",
      "Minibatch accuracy: 48.6%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 6740: 23.784473\n",
      "Minibatch accuracy: 46.5%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 6760: 18.653059\n",
      "Minibatch accuracy: 52.2%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 6780: 21.776018\n",
      "Minibatch accuracy: 48.6%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 6800: 23.712145\n",
      "Minibatch accuracy: 46.5%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 6820: 18.591475\n",
      "Minibatch accuracy: 52.2%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 6840: 21.701622\n",
      "Minibatch accuracy: 48.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6860: 23.639759\n",
      "Minibatch accuracy: 46.3%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 6880: 18.529871\n",
      "Minibatch accuracy: 52.4%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 6900: 21.627684\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6920: 23.567762\n",
      "Minibatch accuracy: 46.3%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 6940: 18.468775\n",
      "Minibatch accuracy: 52.4%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 6960: 21.553715\n",
      "Minibatch accuracy: 48.5%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 6980: 23.496246\n",
      "Minibatch accuracy: 46.5%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 7000: 18.408239\n",
      "Minibatch accuracy: 52.4%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 7020: 21.479647\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 7040: 23.425156\n",
      "Minibatch accuracy: 46.8%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 7060: 18.347824\n",
      "Minibatch accuracy: 52.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7080: 21.405586\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 7100: 23.354519\n",
      "Minibatch accuracy: 46.8%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7120: 18.287632\n",
      "Minibatch accuracy: 52.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7140: 21.332403\n",
      "Minibatch accuracy: 48.7%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 7160: 23.284294\n",
      "Minibatch accuracy: 46.5%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 7180: 18.227703\n",
      "Minibatch accuracy: 52.4%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 7200: 21.260080\n",
      "Minibatch accuracy: 48.7%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 7220: 23.213985\n",
      "Minibatch accuracy: 46.5%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 7240: 18.167891\n",
      "Minibatch accuracy: 52.4%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 7260: 21.188274\n",
      "Minibatch accuracy: 48.7%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 7280: 23.144217\n",
      "Minibatch accuracy: 46.5%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 7300: 18.108692\n",
      "Minibatch accuracy: 52.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7320: 21.117128\n",
      "Minibatch accuracy: 48.4%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 7340: 23.074800\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7360: 18.050404\n",
      "Minibatch accuracy: 52.2%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 7380: 21.046694\n",
      "Minibatch accuracy: 48.3%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7400: 23.005640\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 7420: 17.992504\n",
      "Minibatch accuracy: 52.2%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 7440: 20.976778\n",
      "Minibatch accuracy: 48.3%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7460: 22.936249\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 7480: 17.934996\n",
      "Minibatch accuracy: 52.2%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 7500: 20.906939\n",
      "Minibatch accuracy: 47.8%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7520: 22.867222\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 7540: 17.878235\n",
      "Minibatch accuracy: 52.2%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7560: 20.837044\n",
      "Minibatch accuracy: 47.8%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7580: 22.798567\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7600: 17.822050\n",
      "Minibatch accuracy: 52.2%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7620: 20.768066\n",
      "Minibatch accuracy: 47.8%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7640: 22.729925\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7660: 17.765610\n",
      "Minibatch accuracy: 52.2%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7680: 20.699749\n",
      "Minibatch accuracy: 47.8%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 7700: 22.661003\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7720: 17.709774\n",
      "Minibatch accuracy: 52.2%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7740: 20.632198\n",
      "Minibatch accuracy: 47.8%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 7760: 22.592512\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7780: 17.654196\n",
      "Minibatch accuracy: 52.0%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7800: 20.565605\n",
      "Minibatch accuracy: 47.8%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 7820: 22.525148\n",
      "Minibatch accuracy: 46.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7840: 17.598854\n",
      "Minibatch accuracy: 52.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7860: 20.499962\n",
      "Minibatch accuracy: 48.1%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 7880: 22.457481\n",
      "Minibatch accuracy: 46.3%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7900: 17.544004\n",
      "Minibatch accuracy: 52.0%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 7920: 20.434679\n",
      "Minibatch accuracy: 48.0%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 7940: 22.390003\n",
      "Minibatch accuracy: 46.3%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 7960: 17.489649\n",
      "Minibatch accuracy: 51.9%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 7980: 20.369814\n",
      "Minibatch accuracy: 48.0%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 8000: 22.323355\n",
      "Minibatch accuracy: 46.3%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8020: 17.435389\n",
      "Minibatch accuracy: 51.9%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8040: 20.305281\n",
      "Minibatch accuracy: 48.1%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 8060: 22.257141\n",
      "Minibatch accuracy: 46.3%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 8080: 17.381695\n",
      "Minibatch accuracy: 51.8%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8100: 20.241045\n",
      "Minibatch accuracy: 48.2%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 8120: 22.191273\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8140: 17.328434\n",
      "Minibatch accuracy: 51.8%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8160: 20.177242\n",
      "Minibatch accuracy: 48.2%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 8180: 22.125713\n",
      "Minibatch accuracy: 46.0%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8200: 17.275772\n",
      "Minibatch accuracy: 51.4%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8220: 20.113817\n",
      "Minibatch accuracy: 48.2%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 8240: 22.060175\n",
      "Minibatch accuracy: 46.0%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8260: 17.223625\n",
      "Minibatch accuracy: 51.4%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8280: 20.051012\n",
      "Minibatch accuracy: 48.2%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 8300: 21.995073\n",
      "Minibatch accuracy: 46.0%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8320: 17.171978\n",
      "Minibatch accuracy: 51.0%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 8340: 19.988714\n",
      "Minibatch accuracy: 48.0%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 8360: 21.930336\n",
      "Minibatch accuracy: 46.3%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8380: 17.120897\n",
      "Minibatch accuracy: 51.0%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8400: 19.926895\n",
      "Minibatch accuracy: 48.0%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 8420: 21.865961\n",
      "Minibatch accuracy: 46.3%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8440: 17.070221\n",
      "Minibatch accuracy: 50.7%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8460: 19.865675\n",
      "Minibatch accuracy: 48.1%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 8480: 21.802147\n",
      "Minibatch accuracy: 46.0%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8500: 17.020355\n",
      "Minibatch accuracy: 50.7%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8520: 19.805220\n",
      "Minibatch accuracy: 48.1%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 8540: 21.738430\n",
      "Minibatch accuracy: 46.3%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8560: 16.970980\n",
      "Minibatch accuracy: 50.7%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8580: 19.745691\n",
      "Minibatch accuracy: 48.1%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 8600: 21.675148\n",
      "Minibatch accuracy: 46.2%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8620: 16.921995\n",
      "Minibatch accuracy: 50.5%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8640: 19.686676\n",
      "Minibatch accuracy: 48.1%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8660: 21.612141\n",
      "Minibatch accuracy: 46.2%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8680: 16.873384\n",
      "Minibatch accuracy: 50.6%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8700: 19.628056\n",
      "Minibatch accuracy: 47.8%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 8720: 21.549234\n",
      "Minibatch accuracy: 46.2%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8740: 16.824888\n",
      "Minibatch accuracy: 50.8%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8760: 19.569977\n",
      "Minibatch accuracy: 47.6%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 8780: 21.486404\n",
      "Minibatch accuracy: 46.5%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8800: 16.776863\n",
      "Minibatch accuracy: 50.6%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8820: 19.512548\n",
      "Minibatch accuracy: 47.6%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8840: 21.423721\n",
      "Minibatch accuracy: 46.4%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8860: 16.729782\n",
      "Minibatch accuracy: 50.4%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8880: 19.455507\n",
      "Minibatch accuracy: 47.6%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8900: 21.361235\n",
      "Minibatch accuracy: 46.7%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8920: 16.683311\n",
      "Minibatch accuracy: 50.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 8940: 19.398813\n",
      "Minibatch accuracy: 47.6%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8960: 21.298933\n",
      "Minibatch accuracy: 46.7%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 8980: 16.637230\n",
      "Minibatch accuracy: 50.1%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 9000: 19.342628\n",
      "Minibatch accuracy: 47.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 9020: 21.236723\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 9040: 16.591496\n",
      "Minibatch accuracy: 50.1%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 9060: 19.286558\n",
      "Minibatch accuracy: 47.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 9080: 21.175177\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 9100: 16.546017\n",
      "Minibatch accuracy: 49.9%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 9120: 19.231083\n",
      "Minibatch accuracy: 47.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 9140: 21.113710\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 9160: 16.500841\n",
      "Minibatch accuracy: 50.4%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 9180: 19.175982\n",
      "Minibatch accuracy: 47.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 9200: 21.052465\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 9220: 16.455986\n",
      "Minibatch accuracy: 50.4%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 9240: 19.120985\n",
      "Minibatch accuracy: 47.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 9260: 20.991402\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 9280: 16.411516\n",
      "Minibatch accuracy: 50.1%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 9300: 19.066799\n",
      "Minibatch accuracy: 47.8%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 9320: 20.930620\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 9340: 16.367275\n",
      "Minibatch accuracy: 50.1%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 9360: 19.012918\n",
      "Minibatch accuracy: 47.3%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 9380: 20.870108\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 9400: 16.323322\n",
      "Minibatch accuracy: 50.1%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 9420: 18.959265\n",
      "Minibatch accuracy: 47.3%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 9440: 20.809856\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 9460: 16.279903\n",
      "Minibatch accuracy: 50.1%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 9480: 18.905849\n",
      "Minibatch accuracy: 47.3%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 9500: 20.749821\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 9520: 16.236753\n",
      "Minibatch accuracy: 50.1%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 9540: 18.852854\n",
      "Minibatch accuracy: 46.8%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 9560: 20.689919\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 9580: 16.193766\n",
      "Minibatch accuracy: 50.4%\n",
      "Validation accuracy: 49.9%\n",
      "Minibatch loss at step 9600: 18.800211\n",
      "Minibatch accuracy: 46.8%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 9620: 20.629913\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 9640: 16.150671\n",
      "Minibatch accuracy: 50.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 9660: 18.747816\n",
      "Minibatch accuracy: 47.0%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 9680: 20.570234\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 9700: 16.107702\n",
      "Minibatch accuracy: 50.4%\n",
      "Validation accuracy: 49.8%\n",
      "Minibatch loss at step 9720: 18.695543\n",
      "Minibatch accuracy: 47.0%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 9740: 20.510885\n",
      "Minibatch accuracy: 46.6%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 9760: 16.064817\n",
      "Minibatch accuracy: 50.4%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 9780: 18.643667\n",
      "Minibatch accuracy: 47.0%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 9800: 20.451996\n",
      "Minibatch accuracy: 46.4%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 9820: 16.022179\n",
      "Minibatch accuracy: 50.4%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 9840: 18.592226\n",
      "Minibatch accuracy: 47.0%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 9860: 20.393509\n",
      "Minibatch accuracy: 46.2%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 9880: 15.979555\n",
      "Minibatch accuracy: 50.1%\n",
      "Validation accuracy: 49.7%\n",
      "Minibatch loss at step 9900: 18.541199\n",
      "Minibatch accuracy: 47.0%\n",
      "Validation accuracy: 49.5%\n",
      "Minibatch loss at step 9920: 20.335340\n",
      "Minibatch accuracy: 45.9%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 9940: 15.936909\n",
      "Minibatch accuracy: 50.1%\n",
      "Validation accuracy: 49.6%\n",
      "Minibatch loss at step 9960: 18.490501\n",
      "Minibatch accuracy: 47.0%\n",
      "Validation accuracy: 49.4%\n",
      "Minibatch loss at step 9980: 20.277420\n",
      "Minibatch accuracy: 45.9%\n",
      "Validation accuracy: 49.5%\n",
      "Test accuracy: 47.5%\n"
     ]
    }
   ],
   "source": [
    "num_step=10000\n",
    "#def accuracy(predictions, labels):\n",
    "#  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "#          / predictions.shape[0])\n",
    "def accuracy(predictions, labels):\n",
    "    pre=np.argmax(predictions, 1.)\n",
    "    lab=np.argmax(labels, 1.)\n",
    "#    print pre[:100]\n",
    "#    print lab[:100]\n",
    "    posi=np.sum(pre*lab).astype(np.float32)/np.sum(lab).astype(np.float32)\n",
    "    neg=np.sum((1.-pre)*(1.-lab)).astype(np.float32)/np.sum(1-lab).astype(np.float32)\n",
    "    return 100.*2.*posi*neg/(posi+neg)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    for i in range(num_step):\n",
    "        offset=i*batch_size % train_num\n",
    "        train_set_feed=train_set[offset:offset+batch_size,:]\n",
    "        train_lable_feed=train_lable[offset:offset+batch_size]\n",
    "        feed_dict={train_set_batch:train_set_feed, train_lable_batch:train_lable_feed}\n",
    "        _,l,pred=session.run([optimizer,loss,train_pred],feed_dict=feed_dict)\n",
    "        if (i % 20)==0:\n",
    "            print(\"Minibatch loss at step %d: %f\" % (i, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(pred, train_lable_feed))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_pred.eval(), valid_lable))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_pred.eval(), test_lable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 10)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
