{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yueyuchen/anaconda/envs/tensorflow/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load data,channge type\n",
    "load = np.genfromtxt(\"/Users/yueyuchen/Documents/ML/ML_Immune/Machine_Learning_Immunogenicity/data/bcell_peptide_nodups.csv\",dtype = str, delimiter = ',')\n",
    "\n",
    "length1 = np.shape(load)[0]\n",
    "labels1 = np.zeros(length1)\n",
    "load1 = np.zeros(length1)\n",
    "for i in range(length1):\n",
    "    labels1[i] = (load[i,1] == '1')\n",
    "for i in range(length1):   \n",
    "    load1[i] = len(load[i,0])\n",
    "data = []\n",
    "labels = []\n",
    "for i in range(length1):\n",
    "    if(load1[i] < 20):\n",
    "        data.append(load[i,0])\n",
    "        labels.append(labels1[i])\n",
    "\n",
    "length = len(data)\n",
    "\n",
    "Alph = {'A':1,'a':1,\n",
    "        'B':2,'b':2,\n",
    "        'C':3,'c':3,\n",
    "        'D':4,'d':4,\n",
    "        'E':5,'e':5,\n",
    "        'F':6,'f':6,\n",
    "        'G':7,'g':7,\n",
    "        'H':8,'h':8,\n",
    "        'I':9,'i':9,\n",
    "        'J':10,'j':10,\n",
    "        'K':11,'k':11,\n",
    "        'L':12,'l':12,\n",
    "        'M':13,'m':13,\n",
    "        'N':14,'n':14,\n",
    "        'O':15,'o':15,\n",
    "        'P':16,'p':16,\n",
    "        'Q':17,'q':17,\n",
    "        'R':18,'r':18,\n",
    "        'S':19,'s':19,\n",
    "        'T':20,'t':20,\n",
    "        'U':21,'u':21,\n",
    "        'V':22,'v':22,\n",
    "        'W':23,'w':23,\n",
    "        'X':24,'x':24,\n",
    "        'Y':25,'y':25,\n",
    "        'Z':26,'z':26\n",
    "        }\n",
    "        \n",
    "dataset = []\n",
    "for d in data:\n",
    "    d1 = []\n",
    "    for l in d:\n",
    "        d1.append(np.float32(Alph[l]))\n",
    "    for j in range(20-len(d1)):\n",
    "        d1.append(np.float32(0))\n",
    "    dataset.append(d1)\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if (labels[i] == 0):\n",
    "        labels[i] = [1,0]\n",
    "    else:\n",
    "        labels[i] = [0,1]\n",
    "        \n",
    "for i in range(length):\n",
    "    dataset[i] = np.asarray(dataset[i])\n",
    "    labels[i] = np.float32(np.asarray(labels[i]))\n",
    "\n",
    "dataset = np.asarray(dataset)\n",
    "labels = np.asarray(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datamax = np.amax(dataset,axis = 0)\n",
    "datamin = np.amin(dataset,axis = 0)\n",
    "datarange = datamax - datamin\n",
    "for i in range(len(datarange)):\n",
    "    if(datarange[i] == 0):\n",
    "        datarange[i] = 1\n",
    "dataavg = np.mean(dataset,axis = 0)\n",
    "for i in range(len(dataset)):\n",
    "    dataset[i] = (dataset[i]-dataavg)/datarange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(237603, 20) (237603, 2)\n"
     ]
    }
   ],
   "source": [
    "#shuffle dataset\n",
    "shuf = np.append(dataset,labels,axis = 1)\n",
    "np.random.shuffle(shuf)\n",
    "\n",
    "dataset = shuf[:,:20]\n",
    "labels = shuf[:,20:22]\n",
    "print (np.shape(dataset),np.shape(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainnum = 150000\n",
    "testnum = 10000\n",
    "\n",
    "train_dataset = dataset[:trainnum]\n",
    "train_labels = labels[:trainnum]\n",
    "\n",
    "test_dataset = dataset[trainnum:trainnum + testnum]\n",
    "test_labels = labels[trainnum :trainnum + testnum]\n",
    "\n",
    "\n",
    "\n",
    "valid_dataset = dataset[trainnum + testnum:trainnum + 2*testnum]\n",
    "valid_labels = labels[trainnum + testnum:trainnum + 2*testnum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_hidden_nodes = 20\n",
    "peplength = 20\n",
    "num_labels = 2\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, peplength))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_test_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(testnum, peplength))\n",
    "    tf_test_labels = tf.placeholder(tf.float32, shape=(testnum, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "     \n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "    weights1 = tf.Variable(\n",
    "      tf.truncated_normal([peplength, num_hidden_nodes]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "      tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "    lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "    tlogits = tf.matmul(lay1_train, weights2) + biases2\n",
    "    \n",
    "    lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "    vlogits = tf.matmul(lay1_valid, weights2) + biases2\n",
    "    \n",
    "    tloss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(tlogits, tf_train_labels))+beta_regul * (tf.nn.l2_loss(weights1)+tf.nn.l2_loss(weights2))\n",
    "    \n",
    "    vloss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(vlogits, valid_labels))+beta_regul * (tf.nn.l2_loss(weights1)+tf.nn.l2_loss(weights2))\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.0019).minimize(tloss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(tlogits)\n",
    "    \n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "    lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 1.065276\n",
      "Minibatch accuracy: 68.2%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 500: 0.797142\n",
      "Minibatch accuracy: 81.8%\n",
      "Validation accuracy: 82.3%\n",
      "Minibatch loss at step 1000: 0.701431\n",
      "Minibatch accuracy: 85.8%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 1500: 0.520804\n",
      "Minibatch accuracy: 89.9%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 2000: 0.570494\n",
      "Minibatch accuracy: 88.2%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 2500: 0.528494\n",
      "Minibatch accuracy: 88.6%\n",
      "Validation accuracy: 88.1%\n",
      "Minibatch loss at step 3000: 0.525758\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 3500: 0.586460\n",
      "Minibatch accuracy: 86.5%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 4000: 0.499962\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 4500: 0.500110\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 5000: 0.496346\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 5500: 0.552354\n",
      "Minibatch accuracy: 87.6%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 6000: 0.490856\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 6500: 0.467440\n",
      "Minibatch accuracy: 89.0%\n",
      "Validation accuracy: 89.2%\n",
      "Minibatch loss at step 7000: 0.471550\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 7500: 0.441441\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 8000: 0.463380\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 8500: 0.455063\n",
      "Minibatch accuracy: 89.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 9000: 0.464265\n",
      "Minibatch accuracy: 89.3%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 9500: 0.458104\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 10000: 0.460618\n",
      "Minibatch accuracy: 89.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 10500: 0.468680\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 11000: 0.454293\n",
      "Minibatch accuracy: 89.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 11500: 0.439920\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 12000: 0.445984\n",
      "Minibatch accuracy: 90.1%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 12500: 0.421177\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 13000: 0.490456\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 13500: 0.437129\n",
      "Minibatch accuracy: 89.6%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 14000: 0.431068\n",
      "Minibatch accuracy: 90.5%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 14500: 0.461982\n",
      "Minibatch accuracy: 89.3%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 15000: 0.429272\n",
      "Minibatch accuracy: 90.7%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 15500: 0.464238\n",
      "Minibatch accuracy: 89.5%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 16000: 0.413800\n",
      "Minibatch accuracy: 91.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 16500: 0.463158\n",
      "Minibatch accuracy: 88.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 17000: 0.456710\n",
      "Minibatch accuracy: 89.6%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 17500: 0.435330\n",
      "Minibatch accuracy: 89.7%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 18000: 0.441387\n",
      "Minibatch accuracy: 90.1%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 18500: 0.449110\n",
      "Minibatch accuracy: 89.3%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 19000: 0.443381\n",
      "Minibatch accuracy: 89.0%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 19500: 0.412078\n",
      "Minibatch accuracy: 90.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 20000: 0.482707\n",
      "Minibatch accuracy: 88.5%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 20500: 0.406386\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 21000: 0.449409\n",
      "Minibatch accuracy: 89.2%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 21500: 0.402785\n",
      "Minibatch accuracy: 90.4%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 22000: 0.433699\n",
      "Minibatch accuracy: 88.9%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 22500: 0.446473\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 23000: 0.389941\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 23500: 0.376735\n",
      "Minibatch accuracy: 91.6%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 24000: 0.435374\n",
      "Minibatch accuracy: 89.9%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 24500: 0.388815\n",
      "Minibatch accuracy: 90.3%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 25000: 0.437258\n",
      "Minibatch accuracy: 89.4%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 25500: 0.429695\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 26000: 0.426468\n",
      "Minibatch accuracy: 90.1%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 26500: 0.416674\n",
      "Minibatch accuracy: 90.1%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 27000: 0.421574\n",
      "Minibatch accuracy: 90.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 27500: 0.392018\n",
      "Minibatch accuracy: 91.7%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 28000: 0.410526\n",
      "Minibatch accuracy: 91.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 28500: 0.417157\n",
      "Minibatch accuracy: 89.9%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 29000: 0.386125\n",
      "Minibatch accuracy: 90.9%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 29500: 0.399223\n",
      "Minibatch accuracy: 90.7%\n",
      "Validation accuracy: 90.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 30000\n",
    "train_curv = np.zeros(0)\n",
    "valid_curv = np.zeros(0)\n",
    "test_result = np.zeros(0)\n",
    "\n",
    "tloss_curv = np.zeros(0)\n",
    "vloss_curv = np.zeros(0)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    \n",
    "    tf.initialize_all_variables().run()\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        \n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, \n",
    "                     tf_test_dataset: test_dataset,tf_test_labels:test_labels,beta_regul: 1e-3}\n",
    "        _, tl, vl, predictions = session.run(\n",
    "          [optimizer, tloss, vloss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, tl))\n",
    "            ac = accuracy(predictions, batch_labels)\n",
    "            train_curv = np.append(train_curv,ac)\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % ac)\n",
    "            ac = accuracy(valid_prediction.eval(), valid_labels)\n",
    "            valid_curv = np.append(valid_curv,ac)\n",
    "            print(\"Validation accuracy: %.1f%%\" % ac)\n",
    "            tloss_curv = np.append(tloss_curv,tl)\n",
    "            vloss_curv = np.append(vloss_curv,vl)\n",
    "        \n",
    "    for i in range(15):\n",
    "        test_dataset = dataset[i*testnum:i*testnum + testnum]\n",
    "        test_labels = labels[i*testnum:i*testnum + testnum]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, \n",
    "                     tf_test_dataset: test_dataset, tf_test_labels:test_labels, beta_regul: 1e-3}\n",
    "        testpre = session.run(\n",
    "          test_prediction, feed_dict=feed_dict)\n",
    "        ac = accuracy(testpre,test_labels)\n",
    "        test_result = np.append(test_result,ac)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = np.arange(60)\n",
    "#plt.plot(c,nodevsperformance[0],'r--',c,nodevsperformance[1],'r--',\n",
    "#        c,nodevsperformance[5],'bs',c,nodevsperformance[6],'bs',\n",
    " #       c,nodevsperformance[10],'g^',c,nodevsperformance[11],'g^')\n",
    "plt.plot(c,nodevsperformance[2],'r--',label = 'train&validation accuracy(20nodes)')\n",
    "plt.plot(c,nodevsperformance[3],'r--')\n",
    "\n",
    "plt.plot(c,nodevsperformance[7],'bs',label = 'train&validation accuracy(50nodes)')\n",
    "plt.plot(c,nodevsperformance[8],'bs')\n",
    "\n",
    "plt.plot(c,nodevsperformance[12],'g^',label = 'train&validation accuracy(100nodes)')\n",
    "plt.plot(c,nodevsperformance[13],'g^')\n",
    "\n",
    "\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('prediction accuracy')\n",
    "plt.title('prediction accuracy vs NN nodes')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 89.7 ,  90.01,  90.02,  89.78,  90.39,  90.6 ,  90.4 ,  89.96,\n",
       "        90.74,  90.13,  90.46,  90.3 ,  90.46,  90.78,  89.89])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodenum = []\n",
    "nodevsperformance = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nodenum.append(50)\n",
    "nodevsperformance.append(tloss_curv)\n",
    "nodevsperformance.append(vloss_curv)\n",
    "nodevsperformance.append(train_curv)\n",
    "nodevsperformance.append(valid_curv)\n",
    "nodevsperformance.append(test_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tloss_curv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('/Users/yueyuchen/Documents/ML/ML_Immune/Machine_Learning_Immunogenicity/data/nodedatanew',nodevsperformance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
